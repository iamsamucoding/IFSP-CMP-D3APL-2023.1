{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09142599-d99c-4b84-ae6b-4ee2abc7df3a",
   "metadata": {},
   "source": [
    "**D3APL: Aplicações em Ciência de Dados** <br/>\n",
    "IFSP Campinas\n",
    "\n",
    "Prof. Dr. Samuel Martins (Samuka) <br/><br/>\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbed1c92-5c77-4898-8cbb-e847af6f72cc",
   "metadata": {},
   "source": [
    "# Animal Dataset - v0\n",
    "We will evaluate some **multiclass classification** CNNs to predict the classes of the **Animal Dataset**: https://www.kaggle.com/datasets/alessiocorrado99/animals10\n",
    "\n",
    "\n",
    "Target goals:\n",
    "- Dataset Organization\n",
    "    + Understand the dataset's structure\n",
    "    + Handle the _class imbalance_ by _undersampling_\n",
    "    + Saving the balanced dataset\n",
    "    + Code a function to load the _animals dataset_ as a _dataframe_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25efbd0-4d14-4f97-84f7-18e6e103a6b9",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "**Animal Dataset**: https://www.kaggle.com/datasets/alessiocorrado99/animals10\n",
    "\n",
    "Dataset locally stored on _'../datasets/animal_dataset'_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5e324b-4a1d-48ae-9458-96ea4a7290ff",
   "metadata": {},
   "source": [
    "### 1.1 Dataset Info\n",
    "**Dataset Folder:** `../datasets/animals-dataset/raw-img`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafc1097-ceb7-42b2-b5a7-46d26d8287fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468bccbc-d785-48d8-ab46-0f273de4c220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa3a8607-4f31-488f-8dd7-6afce07ab9e8",
   "metadata": {},
   "source": [
    "Each **animal** (a **class** in your _classification problem_) has a _folder_ inside the _dataset folder_. <br/>\n",
    "There are **10 animals (classes)** in the dataset.\n",
    "\n",
    "Each _class folder_ contain _all_ **images** (**samples**) from the corresponding _class_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b176668-8f5d-4832-88fd-35f189d34425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23bd4e6b-b762-4a73-ad30-39b954f64732",
   "metadata": {},
   "source": [
    "We see that it is an **imbalanced dataset**. \n",
    "\n",
    "The class with the _fewest images_ is _\"elefante\"_ (1446 images) and the one with the most images is _\"cane\"_ (4863 images)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638f686e-a6e4-485b-97cf-647724e8e296",
   "metadata": {},
   "source": [
    "### 1.2 Handling Class Imbalance by Undersampling\n",
    "To handle the **class imbalance** we will consider the **_undersampling_** technique:\n",
    "\n",
    "<img src='./figs/undersampling_x_oversampling.jpg' width=600/> <br/>\n",
    "Source: https://www.mastersindatascience.org/learning/statistics-data-science/undersampling/#:~:text=Undersampling%20is%20a%20technique%20to,information%20from%20originally%20imbalanced%20datasets.\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ed6a14-4f2b-4f2f-be57-2e6c9716a5d3",
   "metadata": {},
   "source": [
    "Although _the lowest number of samples per class_ in our dataset is _1446 images_, we will consider a \"rounded number\": **1400 images**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd23554-3758-4b5b-92c8-b2f284f4ab7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee07b2fe-455a-4481-a4d7-814cfbd442da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ita ==> en\n",
    "translate = {'cane': 'dog', 'cavallo': 'horse', 'elefante': 'elephant', 'farfalla': 'butterfly', 'gallina': 'chicken', 'gatto': 'cat', 'mucca': 'cow', 'pecora': 'sheep', 'scoiattolo': 'squirrel', 'ragno': 'spider'}\n",
    "\n",
    "translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1734da-c42f-4eb5-b88e-dfaeec903ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914502f3-bcd8-41d3-adc4-9ad3b299a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe to store the image full pathnames and their corresponding classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2247499f-099b-4c87-916f-a2b04c310c36",
   "metadata": {},
   "source": [
    "##### **Alternative to Undersampling from Scratch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b63b3c1-f9ce-4ea5-9500-f625b2d0842c",
   "metadata": {},
   "source": [
    "The [`imbalanced-learn` package](https://imbalanced-learn.org/stable/references/index.html) provides several functionalities to deal with **class_imbalance**, such as _undersampling_ and _oversampling_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff9f738-f843-40b8-bc08-65a6a672e153",
   "metadata": {},
   "source": [
    "### 1.3 Saving the undersampled dataset\n",
    "We could create a new folder for the balanced dataset and only save the selected images. <br/>\n",
    "Instead, let's simply save the dataframe as a csv for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dd86d4-100a-4438-99f0-0e7ff22ee01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.to_csv('../datasets/animals-dataset/animals_dataset_balanced.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f0a7f-4913-47e3-8e28-b4301c4a4799",
   "metadata": {},
   "source": [
    "##### ATTENTION\n",
    "The _image pathnames_ shown in the the CSV contain a _relative path_ according to the directory of this notebook. <br/>\n",
    "If you try to open some image from a notebook started on other location, an error will appear. <br/>\n",
    "One solution is to save the _absolute path_ of each image or simply _adjust__ the relative path according to your need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e0cc7d-18a2-48fe-8196-d152c1b3b58d",
   "metadata": {},
   "source": [
    "### 1.4 Inspect some images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69500a3f-71d9-4f1d-b46e-847fd245968e",
   "metadata": {},
   "source": [
    "**Open CV** for Image Processing: https://pypi.org/project/opencv-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23a0ce8-d44f-4475-815f-54df8ca36da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cc828ea-0224-4725-b3f4-029a99b78d54",
   "metadata": {},
   "source": [
    "**Read image:** https://www.askpython.com/python-modules/python-imread-opencv#:~:text=Return%20Value%3A%20cv2.,%2C%20unsupported%20or%20invalid%20format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c8be0e-4046-4a0d-b0a7-452ecc41c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read an image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8102fcfe-60ad-4e2f-9d78-569dd7db5a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1e2510-9563-4fff-a462-d0d699c18a71",
   "metadata": {},
   "source": [
    "**Image's Shape**\n",
    "- **Color Image:** (_height_, _width_, _channels_)\n",
    "- **Channel Order:**  **BGR**: Blue, Green, Red ==> flag value is `cv2.IMREAD_COLOR` in `imread()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9966c13a-27d8-490a-a6da-c50e388b5360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6079444a-449f-4761-8dd3-38f064b6de95",
   "metadata": {},
   "source": [
    "**Image depth (number of bits)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41249d10-8b10-4bcb-b342-e1bdb0b55646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "934046f4-2b79-46e5-a9f0-5095bfedb627",
   "metadata": {},
   "source": [
    "It's an 24-bit color image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2894e0-3ee3-418e-bf23-a0cac1d84b9c",
   "metadata": {},
   "source": [
    "**Visualizing the image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaa76b2-a234-4312-9859-424af0ad3b73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bc585eb-7a95-44b5-9af8-fdbdb8191790",
   "metadata": {},
   "source": [
    "Note that the **color channels** are in a _different order_. <br/>\n",
    "We need to _reorganize the channels_ from **BGR** to **RGB**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc184b4-eb3d-4e6e-9980-ec611c53e246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d273a3-55f0-4efd-adf8-22c1282f13bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f2d6539-9bd9-48b6-b69d-c370139699e6",
   "metadata": {},
   "source": [
    "**Other image:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f493e845-f582-438d-a05e-dde43b87702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(dataset_df.loc[9999, 'image_pathname'])\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fff1ed-485d-48e1-b856-0c313ea85089",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee620002-2e82-4c34-9749-2ef248516ba4",
   "metadata": {},
   "source": [
    "Note that the **images' shapes are different**, so we will need to **rescale the images** to a _standard shape_ according to the _considered network's architecture_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71334459-9465-44cd-bac0-0f919650a939",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.min(), img.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef0906-7a68-408a-86df-33cd41b8b3b9",
   "metadata": {},
   "source": [
    "It's an 24-bit color image."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
